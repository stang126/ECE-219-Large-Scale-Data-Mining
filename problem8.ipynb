{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Data\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize, pos_tag\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "import seaborn as sn\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "df = pd.read_csv('Project1-Classification.csv')\n",
    "\n",
    "# Seed random generator\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Split data into training and testing data\n",
    "train, test = train_test_split(df[[\"full_text\",\"root_label\"]], test_size = 0.2)\n",
    "\n",
    "# Clean Data\n",
    "def clean(text):\n",
    "    text = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "    texter = re.sub(r\"<br />\", \" \", text)\n",
    "    texter = re.sub(r\"&quot;\", \"\\\"\",texter)\n",
    "    texter = re.sub('&#39;', \"\\\"\", texter)\n",
    "    texter = re.sub('\\n', \" \", texter)\n",
    "    texter = re.sub(' u ',\" you \", texter)\n",
    "    texter = re.sub('`',\"\", texter)\n",
    "    texter = re.sub(' +', ' ', texter)\n",
    "    texter = re.sub(r\"(!)\\1+\", r\"!\", texter)\n",
    "    texter = re.sub(r\"(\\?)\\1+\", r\"?\", texter)\n",
    "    texter = re.sub('&amp;', 'and', texter)\n",
    "    texter = re.sub('\\r', ' ',texter)\n",
    "    clean = re.compile('<.*?>')\n",
    "    texter = texter.encode('ascii', 'ignore').decode('ascii')\n",
    "    texter = re.sub(clean, '', texter)\n",
    "    if texter == \"\":\n",
    "        texter = \"\"\n",
    "    return texter\n",
    "\n",
    "table = str.maketrans(dict.fromkeys(string.punctuation))\n",
    "\n",
    "def clean_text(text, table):\n",
    "    for i in list(text.index):\n",
    "        text[i] = clean(text[i])\n",
    "        text[i] = text[i].translate(table)\n",
    "        text[i] = re.sub(r'\\S*\\d\\S*','',text[i])\n",
    "        text[i] = text[i].lower()\n",
    "    return text\n",
    "\n",
    "train.full_text=clean_text(train.full_text, table)\n",
    "test.full_text=clean_text(test.full_text, table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Extraction\n",
    "\n",
    "# Create Lemmatizing function\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def get_pos(tag): \n",
    "    pos_dict = {'JJ':'a', 'NN':'n', 'RB':'r', 'VB':'v'}\n",
    "    if tag[1][:2] in list(pos_dict.keys()):\n",
    "        return pos_dict[tag[1][:2]]\n",
    "    else:\n",
    "        return 'n'\n",
    "    \n",
    "def lemmatize_text(text): # lemmatize text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tags = pos_tag(tokens)\n",
    "    return  [wnl.lemmatize(pair[0],get_pos(pair)) for pair in tags]\n",
    "\n",
    "# Create Stemmer \n",
    "ps = nltk.stem.PorterStemmer()\n",
    "\n",
    "def stem_text(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return [ps.stem(word) for word in tokens]\n",
    "\n",
    "# transform categories into number (Climate=0, Sports=1)\n",
    "Encoder = LabelEncoder()\n",
    "train['binary_root_label']=Encoder.fit_transform(train['root_label'])\n",
    "test['binary_root_label']=Encoder.fit_transform(test['root_label'])\n",
    "\n",
    "# Reduce Dimensionality\n",
    "nmf_5 = NMF(n_components = 5, init='random', random_state=42)\n",
    "nmf_30 = NMF(n_components = 30, init='random', random_state=42)\n",
    "nmf_80 = NMF(n_components = 80, init='random', random_state=42)\n",
    "svd_5 = TruncatedSVD(n_components = 5, random_state=42)\n",
    "svd_30 = TruncatedSVD(n_components = 30, random_state=42)\n",
    "svd_80 = TruncatedSVD(n_components = 80, random_state=42)\n",
    "\n",
    "# Classifiers\n",
    "l1_logregression = LogisticRegression(penalty='l1',random_state=42, solver='saga', max_iter=100000, C=100)\n",
    "l2_logregression = LogisticRegression(penalty='l2',random_state=42, solver='saga', max_iter=100000, C=1000)\n",
    "best_svm = svm.SVC(kernel='linear', C=100000, random_state=42)\n",
    "gnb = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "pipeline = Pipeline([ # SVM with best gamma\n",
    "    ('vect', CountVectorizer(stop_words='english')),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('reduce_dim', NMF(n_components=5, init='random', random_state=42)), # dim reduction\n",
    "    ('clf', svm.SVC(kernel='linear', C=1))])\n",
    "\n",
    "param_grid = {'vect__min_df': (3,5), # set min_df\n",
    "              'vect__analyzer': (lemmatize_text, stem_text),  ## add lemmatization and stemming\n",
    "              'reduce_dim': (nmf_5, nmf_30, nmf_80, svd_5, svd_30, svd_80), # reduce dimensionality\n",
    "              'clf': (l1_logregression, l2_logregression, best_svm, gnb)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = GridSearchCV(pipeline, param_grid, n_jobs=-1, cv=5)\n",
    "search.fit(train.full_text, train['binary_root_label'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6c4d0dcc6ab9b928a81d54ec458645a30fcac0c4b83eb673056319a03dc0ba5e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
